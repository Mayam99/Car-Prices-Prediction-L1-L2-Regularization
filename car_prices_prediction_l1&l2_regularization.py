# -*- coding: utf-8 -*-
"""Car-Prices-Prediction-L1&L2-Regularization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YjhnWLCoNWLeX1fmeKTUBGcY_l372bov

##Car-Prices-Prediction-data
###The Car-Prices-Prediction dataset from Kaggle is designed to help users build models that can accurately predict the prices of cars based on various features. This dataset typically includes attributes like car model, brand, year of manufacture, engine type, fuel type, mileage, horsepower, and other key factors influencing car pricing. The primary goal is to analyze these features and develop a model that estimates the price of a car given the input data.

##Detailed Introduction for the Dataset
###The Car-Prices-Prediction dataset consists of various features that impact the market value of vehicles. The dataset includes information on:

* Make and Model: Identifying the brand and model of the car.
* Year: The year the car was manufactured.
* Mileage (KM or Miles driven): The total distance the car has traveled.
* Engine Specifications: Details like engine size, type (petrol/diesel), and horsepower.
* Transmission Type: Whether the car has manual or automatic transmission.
* Fuel Type: Petrol, diesel, electric, or hybrid fuel options.
* Condition and Age: Older cars typically depreciate in value.
* Additional Features: Extras like leather seats, sunroof, and technology
  upgrades.

###These features combined form a rich dataset suitable for regression analysis. The goal is to predict the car price (target variable) based on these input features.

##Moving Forward with L1 and L2 Regularization
###In this context, the focus is on applying L1 and L2 regularization techniques to improve model performance, particularly in the presence of multicollinearity or irrelevant features. Here’s a recommended approach:

###1. Exploratory Data Analysis (EDA)
###Perform detailed EDA to understand the dataset, including:
* Checking for missing values.
* Visualizing relationships between features and the target variable.
* Identifying potential outliers.
* Analyzing feature correlations.

###2. Data Preprocessing
* Handle missing values, if any.
* Encode categorical variables using methods like One-Hot Encoding.
* Scale features using standardization or normalization to prepare the data for
  regularization techniques.

###3. Modeling with Regularization
* L1 Regularization (Lasso Regression): Focuses on reducing the number of features by applying a penalty equal to the absolute value of the coefficients. It can effectively reduce less important features to zero, thus performing feature selection.
* L2 Regularization (Ridge Regression): Applies a penalty equal to the square of the magnitude of coefficients. It’s useful for shrinking coefficients, helping to deal with multicollinearity and improving model generalization.

###4. Implementing and Tuning Models
* Split the data into training and testing sets.
* Fit both Lasso and Ridge regression models.
* Use cross-validation to tune the regularization strength (alpha parameter).
* Compare model performance using metrics like RMSE, R², and MAE.

###5. Evaluating and Interpreting Results
* Assess the performance of Lasso and Ridge models.
* Determine whether L1 or L2 regularization provides better results based on your objectives (e.g., feature selection vs. handling multicollinearity).
* Interpret the coefficients and understand how regularization affects feature importance.

###This structured approach allows you to leverage the strengths of L1 and L2 regularization, leading to a robust model that performs well on unseen data.

##Importing Necessary Libraries
"""

import pandas as pd # Pandas is a powerful library for data manipulation and analysis.
import numpy as np # NumPy is a powerful tool for numerical computations in Python

"""##Loading the Dataset"""

df= pd.read_csv("CarPricesPrediction.csv") #The df = pd.read_csv line reads a CSV file into a DataFrame named df using the pandas library.

df.head()

df.shape # Displays the total count of the Rows and Columns respectively.

df.info() #Displays the total count of values present in the particular column along with the null count and data type.

print(df.isnull().sum()) #Is used to display the dimensions of the DataFrame df. Giving you a quick overview of the size of your dataset.

df.info() #Displays the total count of values present in the particular column along with the null count and data type.

"""###Dropping Irrelevant Columns: The Unnamed: 0 column appears to be an index and can be dropped since it doesn’t provide any meaningful information."""

df = df.drop(['Unnamed: 0'], axis=1)

"""###Converting Categorical Features: The Make, Model, and Condition columns are categorical and need to be encoded for modeling. You can use one-hot encoding:"""

df = pd.get_dummies(df, columns=['Make', 'Model', 'Condition'], drop_first=True)

df.head() #Checking for the changes.

"""###Feature Scaling: Regularization techniques are sensitive to feature scaling.

The StandardScaler() scales the Year and Mileage features to have a mean of 0 and a standard deviation of 1, ensuring uniform contribution of these features during model training.
"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df[['Year', 'Mileage']] = scaler.fit_transform(df[['Year', 'Mileage']])

"""##Splitting the Data
###Separate the features and target variable, and then split the data into training and testing sets:

The code splits the features (X) and target variable (y) into training and testing sets, with 20% of the data allocated for testing, ensuring consistent results with random_state=42.
"""

from sklearn.model_selection import train_test_split

X = df.drop('Price', axis=1)  # Keeping other Features except for the price and assigning to variable X.
y = df['Price']  # y variable is assigned the Price column as it is the Target variable.

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""##3: Implementing L1 (Lasso) and L2 (Ridge) Regularization
###Lasso (L1 Regularization):
"""

from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV

"""The code performs hyperparameter tuning for a Lasso regression model by using grid search with 5-fold cross-validation (cv=5) to find the best alpha value from the given list ([0.01, 0.1, 1, 10, 100]), which controls the regularization strength. The model is then trained on the training data (X_train, y_train)."""

# Hyperparameter tuning for alpha (regularization strength)
lasso = Lasso()
params = {'alpha': [0.01, 0.1, 1, 10, 100]}
lasso_cv = GridSearchCV(lasso, params, cv=5)
lasso_cv.fit(X_train, y_train)

# Best alpha value
print(f"Best alpha for Lasso: {lasso_cv.best_params_['alpha']}")

"""
The code initializes a Lasso model with the best alpha value found from grid search (lasso_cv.best_params_['alpha']), then trains this model on the training data (X_train, y_train), and finally uses the trained model to predict target values (y_pred) for the test data (X_test)."""

# Model evaluation
lasso_best = Lasso(alpha=lasso_cv.best_params_['alpha'])
lasso_best.fit(X_train, y_train)
y_pred = lasso_best.predict(X_test)

# Evaluate performance
from sklearn.metrics import mean_squared_error, r2_score

"""###The code calculates and prints the Lasso model's performance metrics:

* RMSE (Root Mean Squared Error): Measures the average error between predicted and actual values, indicating model accuracy.
* R² (R-squared): Represents the proportion of variance in the target variable explained by the model, indicating goodness-of-fit.
"""

print("Lasso RMSE:", np.sqrt(mean_squared_error(y_test, y_pred)))
print("Lasso R²:", r2_score(y_test, y_pred))

"""##Ridge (L2 Regularization):"""

from sklearn.linear_model import Ridge

"""The code performs hyperparameter tuning for a Ridge regression model using grid search with 5-fold cross-validation (cv=5). It tests different alpha values ([0.01, 0.1, 1, 10, 100]) to find the best regularization strength. The model is then trained on the training data (X_train, y_train) using the optimal alpha."""

# Hyperparameter tuning for alpha
ridge = Ridge()
params = {'alpha': [0.01, 0.1, 1, 10, 100]}
ridge_cv = GridSearchCV(ridge, params, cv=5)
ridge_cv.fit(X_train, y_train)

"""
The code prints the best alpha value for the Ridge regression model, found during grid search, which represents the optimal regularization strength that balances model complexity and performance."""

# Best alpha value
print(f"Best alpha for Ridge: {ridge_cv.best_params_['alpha']}")

"""The code initializes a Ridge regression model with the best alpha value found from grid search (ridge_cv.best_params_['alpha']), trains the model on the training data (X_train, y_train), and then uses the trained model to predict the target values (y_pred) for the test data (X_test)."""

# Model evaluation
ridge_best = Ridge(alpha=ridge_cv.best_params_['alpha'])
ridge_best.fit(X_train, y_train)
y_pred = ridge_best.predict(X_test)

"""###The code calculates and prints the performance metrics for the Ridge regression model:

* RMSE (Root Mean Squared Error): Shows the average difference between the predicted and actual values, indicating the model's prediction accuracy.
* R² (R-squared): Reflects how well the model explains the variance in the target variable, indicating how well the model fits the data.
"""

# Evaluate performance
print("Ridge RMSE:", np.sqrt(mean_squared_error(y_test, y_pred)))
print("Ridge R²:", r2_score(y_test, y_pred))

